# -*- coding: utf-8 -*-
"""multilingualchatbot.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A_QlS_5ahMwpUPbAmS9dbSUbeL2oOCY3
"""

!pip install accelerate
!pip install HF_TOKEN
!pip install transformers
!pip install huggingface_hub
!pip install gradio
!pip install streamlit as st

import gradio as gr
import transformers

!pip install torchtext

!pip install torchtext

import torch
# from torchtext import get_tokenizer # No longer using torchtext
from collections import Counter
# from torchtext.vocab import build_vocab_from_iterator # No longer using torchtext
from transformers import AutoTokenizer # Using transformers tokenizer

# Sample text data - Using this consistent text for both vocab and data
text = """How do I get certified for a course?
To get certified in a course, you need to successfully complete all quizzes and submissions (if applicable) with 100% progression.


My progression is 100%, but the certificate isn't loading/generating. What should I do?
Please clear your browser history and try again. If the issue persists, log out and log back in, then try again.


I don't know the answer to the quiz of a course, can I still get certified?
Please ensure you complete the quiz with the correct answer. You can attempt the quiz an unlimited number of times.


How can I change my name on my certificate/profile?
Please follow these steps: Go to your profile -> Edit your name -> Refresh your browser (CTRL + F5) -> Regenerate your certificate.


Where can I find the certificate? Where is the 'Get Certified' option?
You need to click on the 'Get Certified' option at the end of the course modules in the table of contents. It will only be displayed after you complete all the lessons with 100% progress.


I cannot download my certificate, a blank screen appears, what should I do?
The certificate should automatically download. Please check your downloads folder, the certificate will download as a PDF file. For better results, try using Google Chrome.


Why is Course X not available in Y language?
We sincerely apologize for the inconvenience. We are committed to offering courses in as many regional languages as possible. Please send your request to info@guvi.in.


What can I do with GEEKOINS?
GUVI users can use GEEKOINS to purchase courses and 'run counts,' useful in CodeKata, WebKata, and Debugging, and redeem coupons from the rewards section.


What is your refund policy? I want a refund for my course.
Refunds are processed within 7 working days from the date you fill out the refund form, which will be sent to your email by our team. To request a refund, please contact us at info@guvi.in or reach out to your course coordinator via email or phone, providing your details and the reason for the refund.


Where can I find the courses?
Please visit this link https://www.guvi.in/courses to view our course library.


I'm not able to log in. Please help.
Please refresh your browser (CTRL + SHIFT + R for Windows, CMD + SHIFT + R for Mac) and try again. Also, consider trying a different browser.



How do I get certified for a course?
We want you to be satisfied, so all eligible courses purchased on GUVI can be refunded within 7 days. For whatever reason, if you are unhappy with a course, you can request a refund, provided the request meets the guidelines in our refund policy. Certain restrictions may apply and some purchases may only be eligible for credit refunds. For more information regarding our refund policy, please see below


How does GUVI's 7-day refund policy work?
While our 7-day refund policy is to allow students to learn risk free, we must also protect our instructors from fraud and provide them a reasonable payment schedule, so we will not process refund requests received after the refund window.


Where will I receive the refund amount?
The refund amount will be transferred to the payment method used by you while purchasing the course.


What are the additional reasons for denied refunds?
We reserve the right, in our sole discretion, to limit or deny refund requests in cases where we believe there is refund abuse, including but not limited to the following:

A significant portion of the course has been consumed or downloaded by a student before the refund was requested.
Multiple refunds have been requested by a student for the same course.
Excessive refunds have been requested by a student.
Users who have their account banned or course access disabled due to a violation of our Terms will not be eligible to receive a refund. We do not grant refunds for any subscription services after 7 days. These refund restrictions will be enforced to the extent permitted by applicable law.


What about refunds for bundles purchased through a third party?
Since the payment for the bundle was processed by the third-party vendor, we do not have the transaction on file and cannot initiate a refund for you. Please contact the third-party vendor directly to request a refund.

More about Professional Development
The benefits of professional development
In today's competitive job market, soft skills often outweigh hard skills when it comes to professional success. While technical expertise is important, it's the mastery of soft skills that truly sets individuals apart. Professional development courses focusing on these skills can be the difference between simply doing a job and excelling in a career.

Soft skills like communication, leadership, and emotional intelligence are the building blocks of a successful professional life. Good interviewing and resume writing skills can enhance your chances of grabbing your dream job by many folds. They enable you to lead teams effectively, manage conflicts with ease, and communicate your ideas with clarity and confidence. In a world where collaboration and adaptability are key, these skills are not just desirable but essential. Employers are increasingly recognizing that technical skills can be taught, but soft skills are what drive long-term success and career growth.

Investing in professional development courses is not just about improving your resume—it's about becoming a well-rounded professional capable of navigating the complexities of the modern workplace. GUVI (Grab Your Vernacular Imprint)
An HCL Group Company | 3M+ Learners | 19 Languages | 1000+ Hiring Companies
Master Tech Skills in your Native Language

Location: Chennai & Noida

Established in 2014 and acquired by the HCL Group in 2022, GUVI is dedicated to providing effective and high-quality learning and skilling programs that transcend language barriers in technology education. GUVI today is trusted by over 3 million learners and 2000+ corporate partners. To learn more, please visit www.guvi.in.

Website
http://www.guvi.in
Verified page
June 16, 2023
Industry
Education Administration Programs
Company size
501-1,000 employees
1,327 associated members LinkedIn members who’ve listed HCL GUVI as their current workplace on their profile.
Headquarters
Chennai, Tamilnadu
Specialties
coding classes, online coding classes, free coding classes, live coding classes, vernacular coding classes, and native language coding classes
It seems that you'd like information about GUVI Geek. However, the provided context and permissions relate specifically to data science courses, and I can only respond to inquiries linked to that course key. If you require specific information or assistance related to your data science course or any other associated details, feel free to ask, and I'll be glad to assist!

If your inquiry about GUVI Geek relates to data science courses they offer, please specify so I can provide the most relevant information. Otherwise, please refer to general sources or directly to their website for information unrelated to the data science course.

Let me know how I can assist you further with your data science course or other related questions!

Is GUVI free or paid?
GUVI - Free Resources. High-Quality Free Online Learning Materials to Upskill your Career! Access a wide range of free resources on Full Stack Development (FSD), UI/UX, Digital Marketing, Data Science, Business Analytics & Digital Marketing (BADM), AI & Machine Learning (AIML), and many more.
Is a GUVI certificate valid?
Yes they are valid! Apart from the certificates you gain through the courses there is a module called Codekata that can actually drive you through hiring opportunities. Learn to code in your native language…
Is GUVI owned by HCL?
Established in 2014 and acquired by the HCL Group in 2022, GUVI is dedicated to providing effective and high-quality learning and skilling programs that transcend language barriers in technology education.
GUVI (Grab Your Vernacular Imprint) Geek Network Private Limited is a leading online learning and skills development company, incubated by IIT Madras and IIM Ahmedabad.

Established in 2014 and acquired by the HCL Group in 2022, GUVI is dedicated to providing effective and high-quality learning and skilling programs that transcend language barriers in technology education. GUVI today is trusted by over 3 million learners and 2000+ corporate partners.

Can I get a free certificate from GUVI?
GUVI's free courses come with globally recognized certificates from top institutions like IIT-Madras. These free courses are the best choice for beginners, students, freshers, or professionals looking to gain a new skill at their own pace, without spending anything.
Top
What is GUVI full form?
GUVI (Grab Your Vernacular Imprint) Geek Network Private Limited is a leading online learning and skills development company, incubated by IIT Madras and IIM Ahmedabad.
Terms of Service
Welcome to GUVI website registered under the name of GUVI GEEK Network Pvt limited. ("GUVI", "Company", "we", or "us"), IITM's Rural Technology and Business Incubator (RTBI) Module #6, I Floor IITM Research Park, Kanagam Road, Taramani, Chennai - 600113. By using GUVI products, software-enabled developer recruitment platform services, or services ("GUVI services"), you agree to the following terms and conditions, and any privacy policies, legal notices, or other applicable policies and guidelines that may be presented to you (collectively, these "Terms of Service") from time to time.

Please read the Terms of Service carefully. If you do not want to agree to these Terms of Service, you must not access the GUVI services.

Acceptance of the GUVI Terms of Service:
Your access to the GUVI services is subject to these Terms of Service. GUVI reserves the right to update and change these Terms of Service from time to time without notice or acceptance by you although if we decide to materially change all or part of the GUVI Terms of Service we will, where reasonably possible, try to give you advance reasonable notice at your email address on record. Such modified Terms of Service will become effective upon the earlier of (i) your continued use of the GUVI service with actual knowledge of such modified Terms of Service, or (ii) thirty (30) days from publication of such modified Terms of Service on this page or the GUVI website.

Your use of the GUVI service is also subject to the GUVI Privacy Policy, which explains how we treat your personal information, and provides information about our data protection practices.

Any use of any of our GUVI fee-based subscription offerings is bound by these Terms of Service plus any additional terms and conditions which will be made available in a separate agreement agreed to by you, and those additional terms become part of your agreement with GUVI if you access that GUVI service.

By signifying your acceptance of these Terms of Service or making any use of the GUVI service, you signify your acceptance of these Terms of Service in effect at the time of your use. If you are an individual acting as a representative of a corporation or other legal entity which wishes to use the GUVI service, then you represent and agree that you have the authority to accept these Terms of Service on behalf of such corporation or other legal entity and that all provisions of these Terms of Service will bind that corporation or other legal entity as if it were named in these Terms of Service in place of you.

Definitions. For the purposes of these Terms of Service, the terms in this Section 2 shall have the meanings assigned to them.

“Candidate” means an individual a User invites to access the GUVI service.

“User” or “Recruiter” means those individuals having registered accounts with a user identification name and password.

“GUVI services” means the software for GUVI coding challenges and user interface software used to recruit and screen software developers that is hosted by GUVI hosting service provider and made available to you pursuant to these Terms of Service.

“Students” to connect with independent contractor instructors (the “Instructors”, collectively with Students, the “Users”) who provide live and recorded instruction, tutoring, and learning services (the “Courses”) via Our Services. The Services include, without limitation, facilitating and hosting Courses and supporting materials, and taking feedback from Users.

From time to time, We may update these Terms to clarify our practices or to reflect new or different practices, such as when We add new features, and GUVI reserves the right in its sole discretion to modify and/or make changes to these Terms at any time. If We make any material change to these Terms, We will notify You using prominent means such as by email notice sent to the email address specified in Your Account or by posting a notice through Our Services. Modifications will become effective on the day they are posted unless stated otherwise.

Account Information and Service Access:
The GUVI service is not available to persons under the age of thirteen (13) or to any Users or Candidates suspended or removed from the GUVI service by GUVI. By using the GUVI service, you represent and warrant that you are at least thirteen (13) years of age and that you are of legal age to form a binding contract and are not a person barred from receiving services under the laws of India or other applicable jurisdiction.

GUVI reserves the right to refuse registration of, or cancel, any account or access to the GUVI services by any User or Candidate in its reasonable discretion, at any time.

GUVI accounts will be accessed through a User ID and password that you will create (your “Credentials”). Your Credentials are solely for your use and must not be shared with other Users. All Users need to have their own account, User ID, and Credentials. You agree that your Credentials and information you provide upon registration and at all other times will be true, accurate, current, and complete (including your name and company name). You are responsible for maintaining the confidentiality of your account password, and for all activities that occur under your account. You agree to immediately notify GUVI of any unauthorized use of your account, password, Credentials, or any other breach of security. You agree that GUVI will not be liable for any loss or damage arising from your failure to provide us with accurate information or to keep your password secure.

Candidates shall access the Interview services when a User invites an individual, via email, to take a technical programming test (“Challenge”). The Candidate will be provided login credentials to access Challenge. Following completion of the Challenge, the Candidate’s access to the GUVI services shall terminate.

By participating in any GUVI promotional event or giving assent to any activity that requires an account on GUVI, your account will be automatically created.

Except as expressly permitted by GUVI, you agree that the GUVI services are to be accessed only for your or your company’s own internal business purposes and not for resale or unauthorized distribution to any third party.

Cookie Policy :
GUVI neither store cookies nor use them for tracking user behavior anonymously. Cookies are only used to provide single sign on in QA platforms for better user experience. Cookies are generated only for the registered users.

The cookies generated not shared or distributed to any partners or third parties, they solely serve the purpose of maintaining sessions within our platform. Information such as user behavior, interests, etc are neither stored nor used for any sort of commercial activities.

Content:
Content. The GUVI service provides certain features, which enables Users and Candidates to submit, post, and share data, text, software, graphics, messages or other material ("Content") on the GUVI service. Content that Users or Candidates submit, post or share is subject to the terms and conditions of these Terms of Service. Content that you own and post on or through the GUVI service belongs to you and you may use it in any way, except as prohibited by applicable state and federal law or regulations. By using the GUVI service, you are granting GUVI and certain third parties permission to use Content as described in these Terms of Service.

Grant of license to GUVI. By using the GUVI service, you are granting GUVI a non-exclusive, worldwide, royalty-free, fully paid-up, sublicenseable, irrevocable and transferable right and license to use, host, store, reproduce, create derivative works of, distribute, modify, display, and communicate your Content on and through the GUVI service. If you post Content on or through the GUVI service, you represent and warrant that you have the right to post that Content and to grant the above rights to GUVI, so ensure that you have the necessary rights to grant GUVI this license for any Content you submit or post to the GUVI service. This license that you grant GUVI is for the limited purpose of operating, promoting, marketing improving and developing GUVI products and services (and derivative works thereof). This license continues even if you terminate your account and/or stop using the GUVI service. GUVI acknowledges that it will not acquire any other rights in any Content that you may upload or post to the GUVI service.

Use of Content. You understand that by posting your Content that you are responsible for your Content and any Content that you create, transmit or display while using the GUVI services and for any consequences thereof. You further agree that under no circumstances shall GUVI be liable for the errors or omissions of another User or Candidate who views your Content. Therefore, you represent and agree to all of the following and acknowledge that GUVI is explicitly relying on such representations and agreement with regard to your Content and use of Content made available on the GUVI service:

The Content represents your own original work and/or you have all necessary rights to disclose the Content. In doing so, you are not violating the rights of any third party and you know of no other individual or entity whose rights will be infringed or violated by the Content being viewed and used as described in these Terms of Service.

You agree to use GUVI services only for purposes that are legal, proper and in accordance with these Terms of Service. Your disclosure of your Content does not violate a confidential relationship with any third party or establish a confidential relationship with GUVI.

You understand that you are responsible for your Content that you upload, post, e-mail, transmit, or otherwise make available through the GUVI service. Except as permitted in these Terms of Service, you do not have the rights to use, reproduce, create derivative works of, distribute, publicly perform or publicly display any Content that does not belong to you, other than viewing of Content or the associated Challenge reports on or through the GUVI service.

Accuracy of Content. GUVI cannot and need not control all Content posted by Candidates or Users on or through the GUVI service, and it does not guarantee the accuracy, integrity or quality of such Content. You agree that under no circumstances will GUVI be liable in any way for any Content, including any errors or omissions in any Content, or any loss or damage of any kind incurred as a result of your use of any Content. You understand that you must evaluate and bear all risks associated with the use of any Content, including any reliance on the Content, integrity, and accuracy of such Content. GUVI does not endorse and is not responsible for the accuracy, usefulness, safety or relating to Content. YOU AGREE TO WAIVE, AND HEREBY DO WAIVE, ANY LEGAL OR EQUITABLE RIGHTS OR REMEDIES YOU HAVE OR MAY HAVE AGAINST GUVI WITH RESPECT THERETO.

Right to Remove or Edit User Content. GUVI reserves the right, in its reasonable discretion, to refuse to allow any Content through the GUVI services, or to edit or remove any Content at any time with or without notice. Without limiting the generality of the preceding sentence, GUVI complies with the Digital Millennium Copyright Act, and will remove Content upon receipt of a compliant takedown notice (see the "Copyright Infringement" section below).

User and Candidate Representations and Warranties. You are solely responsible for your own Content and the consequences of posting or publishing it. In connection with your Content, you affirm, represent, and warrant that: (i) you own, or have the necessary licenses, rights, consents, and permissions to use and authorize GUVI to use all patent, trademark, copyright, or other proprietary rights in and to your Content to enable inclusion and use of your Content in the manner contemplated by GUVI and these Terms of Service, and to grant the rights and license set forth above, and (ii) your Content, GUVI use of such User submissions pursuant to these Terms of Service, and exercise of the license rights set forth above, do not and will not: (a) infringe, violate, or misappropriate any third party right, including any copyright, trademark, patent, trade secret, moral right, Privacy Right, right of publicity, or any other intellectual property or proprietary right; (b) slander, defame, libel, or invade the right of privacy, publicity, or other property rights of any other person; (c) violate any applicable law or regulation; or (d) require obtaining a license from or paying fees or royalties to any third party for the exercise of any rights granted in these Terms of Service, including, by way of example and not limitation, the payment of any royalties to any copyright owners, including any royalties to any agency, collection society, or other entity that administers such rights on behalf of others.

Feedback. If you provide GUVI with any comments, bug reports, feedback, or modifications proposed or suggested by you on the GUVI services ("Feedback"), GUVI shall have the right to use such Feedback at its discretion, including, but not limited to the incorporation of such suggested changes into the GUVI service. You hereby grant GUVI a perpetual, irrevocable, non-exclusive license under all rights necessary to incorporate and use your Feedback for any purpose.

Code of Conduct:
Blocked Content. GUVI has the right, but not the obligation, to remove or block Content from the GUVI service that it determines in its reasonable discretion to be in violation of these Terms of Service, to be unlawful, offensive, threatening, libelous, defamatory, obscene or otherwise objectionable, that violates any party's intellectual property or that is detrimental to the quality or intended spirit of the GUVI service. GUVI also has the right, but not the obligation, to limit or revoke the use privileges of the account of any User or Candidate who posts such Content or engages in such behavior.

Disagreements. You alone are responsible for your interaction with other Users and Candidates via the GUVI service. If you have a dispute with one or more Users or Candidates or if you disagree with the results of a Challenge Report, your sole remedy is to cease using the GUVI service. You irrevocably and forever release GUVI (and GUVI's officers, directors, agents, subsidiaries, contractors, joint ventures and employees) from claims, demands and damages (actual and consequential) of every kind and nature, known and unknown, arising out of or in any way connected with such disputes.

Copyright Infringers. GUVI does not permit copyright infringing activities through the GUVI service and reserves the right to terminate access to the GUVI service and remove all content submitted by any persons who are found to be repeat infringers. Any suspected fraudulent, abusive, or illegal activity that may be grounds for termination of your use of the GUVI service may be referred to appropriate law enforcement authorities. These remedies are in addition to any other remedies GUVI may have at law or in equity.

Inappropriate Content. GUVI will use common business sense regarding behavior or Content allowed on or through the GUVI service. Examples of unacceptable Content or behavior include:

Abuse, harassment, threats, flaming or intimidation of any person or organization.

Uploading or sending to other Users or Candidates pornographic, threatening, embarrassing, hateful, racially or ethnically insulting, libelous, or otherwise inappropriate Content.

Uploading copyrighted material that is not your own or that you do not have the legal right to distribute, display, and otherwise make available to others.

Making unsolicited offers, advertisements, proposals, or sending junk mail (aka spam) to other Users or Candidates.

Impersonating another person or pretending to be affiliated with an organization with which you are not affiliated or misrepresenting the extent of your affiliated or role with an affiliated organization or the source, identity or Content transmitted.

Engaging in or contributing to any illegal activity or activity that violates others’ rights.

Use of derogatory, discriminatory or excessively graphic language.

Transmitting worms, viruses or harmful software.

Disclosing the personal or proprietary information of another person or organization not otherwise permitted by applicable rules or law.

Sharing GUVI accounts, User Credentials or Candidate login credentials with any third party or encouraging any other User or Candidate to do so.

Who We Share Your Data With :
We share certain data about you with instructors, other students, companies performing services for us, our business partners, analytics and data enrichment providers, your social media providers, companies helping us run promotions and surveys, and advertising companies who help us promote our Services. We may also share your data as needed for security, legal compliance, or as part of a corporate restructuring. Lastly, we can share data in other ways such as third parties to provide support throughout the platform such as payment and forum as of now.

When you voluntarily send us an electronic mail, we will keep a record of this information so that we can respond to you. We only collect information from you when you register on our site or fill out a form. Also, when filling out a form on our site, you may be asked to enter your name, e-mail address or phone number. You may, however, visit our site anonymously. In case you have submitted your personal information and contact details, we reserve the rights to Call, SMS, Email or WhatsApp about our products and offers, even if your number has DND activated on it.

Termination:
GUVI, in its reasonable discretion may terminate your account and/or user login credentials and remove and discard any Content within the GUVI service if GUVI believes that you have violated or acted inconsistently with the letter and spirit of these Terms of Service.

You may terminate your account, this Agreement and your right to use the GUVI service at any time and for any reason or no reason, by contacting GUVI user support at info@guvi.in. However, if you have a separate agreement with GUVI which has conflicting terms regarding termination, those terms shall take precedence over this Agreement.

After cancellation or termination of your account for any reason, you will no longer have access to your account and all information and Content in your account or that you have stored on the GUVI service may be, but is not required to be deleted by GUVI. GUVI will have no liability for information or Content that is deleted due to the cancellation or termination of your account for any reason.

Ownership; Proprietary Rights. The GUVI service is owned and operated by GUVI. The visual interfaces, graphics, design, compilation, information, computer code, products, software (including any downloadable software), services, and all other elements of the GUVI service provided by GUVI ("Materials") are protected by Indian copyright, trade dress, patent, and trademark laws, international conventions, and all other relevant intellectual property and proprietary rights, and applicable laws. Except for any third party content or Content uploaded by you, all Materials are the copyrighted property of GUVI or its subsidiaries or affiliated companies and/or third party licensors. All trademarks, service marks, and trade names are proprietary to GUVI or its affiliates and/or third party licensors. Except as expressly authorized by GUVI, you agree not to sell, license, distribute, copy, modify, publicly perform or display, transmit, publish, edit, adapt, create derivative works from, or otherwise make unauthorized use of the Materials.

Third Party Websites and Services. The GUVI services may include links to third party web sites or services ("Linked Sites") solely as a convenience to Users and Candidates. GUVI does not endorse any such Linked Sites or the information, material, products, or services contained on other Linked Sites or accessible through other Linked Sites. Furthermore, GUVI makes no express or implied warranties with regard to the information, material, products, or services that are contained on or accessible through Linked Sites. Access and use of Linked Sites, including the information, material, products, and services on Linked Sites or available through Linked Sites, is solely at your own risk.

Disclaimers/ No Warranties & Limitation of Liability:
EXCEPT AS MAY BE SET FORTH IN ANY SEPARATE SIGNED AGREEMENT BY GUVI AND YOU, THE GUVI SERVICE IS PROVIDED "AS IS" WITH NO REPRESENTATIONS OR WARRANTIES OF ANY KIND, EXPRESS, STATUTORY OR IMPLIED, AS TO THE OPERATION OF THE GUVI SERVICE, OR THE INFORMATION, CONTENT, MATERIALS, OR PRODUCTS INCLUDED ON THE GUVI SERVICE. TO THE FULLEST EXTENT PERMISSIBLE BY APPLICABLE LAW, GUVI AND ITS AFFILIATES, IF ANY, DISCLAIM ALL WARRANTIES, EXPRESS, STATUTORY, OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. FURTHER, GUVI AND ITS AFFILIATES DO NOT WARRANT THE ACCURACY OR COMPLETENESS OF THE INFORMATION, TEXT, GRAPHICS, LINKS OR OTHER INFORMATION CONTAINED IN THE GUVI SERVICE. SOME JURISDICTIONS DO NOT ALLOW LIMITATIONS ON IMPLIED WARRANTIES, SO THE ABOVE LIMITATIONS MAY NOT APPLY TO YOU. EXCEPT AS MAY BE SET FORTH IN ANY SEPARATE SIGNED AGREEMENT BETWEEN GUVI AND A YOU, GUVI DOES NOT WARRANT THAT THE GUVI SERVICE WILL BE AVAILABLE AT ANY TIME OR FROM ANY PARTICULAR LOCATION, WILL BE SECURE OR ERROR-FREE, THAT DEFECTS WILL BE CORRECTED, OR THAT THE GUVI SERVICE IS FREE OF VIRUSES OR OTHER POTENTIALLY HARMFUL COMPONENTS. NO ADVICE OR INFORMATION, WHETHER ORAL OR WRITTEN, OBTAINED FROM GUVI OR THE GUVI SERVICE SHALL CREATE ANY WARRANTY NOT EXPRESSLY STATED IN THESE TERMS OF SERVICE.

EXCEPT AS MAY BE SET FORTH IN ANY SEPARATELY SIGNED AGREEMENT BETWEEN GUVI AND YOU, NEITHER GUVI NOR ITS AFFILIATES WILL BE LIABLE, UNDER ANY THEORY OF LAW, FOR ANY INDIRECT, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, INCLUDING, BUT NOT LIMITED TO LOSS OF PROFITS OR, BUSINESS INTERRUPTION, AND/OR LOSS OF INFORMATION OR DATA. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OR LIMITATION OF INCIDENTAL OR CONSEQUENTIAL DAMAGES, SO THE ABOVE LIMITATIONS AND EXCLUSIONS MAY NOT APPLY TO YOU. NOTWITHSTANDING ANYTHING TO THE CONTRARY CONTAINED HEREIN, GUVI'S MAXIMUM AGGREGATE LIABILITY TO YOU FOR ANY CAUSES WHATSOEVER, AND REGARDLESS OF THE FORM OF ACTION, WILL AT ALL TIMES BE LIMITED TO THE GREATER OF (i) THE AMOUNT PAID, IF ANY, BY YOU TO GUVI FOR THE GUVI SERVICE IN THE SIX (6) MONTHS PRIOR TO THE ACTION GIVING RISE TO LIABILITY OR (ii) ONE HUNDRED DOLLARS ($100.00).

Indemnification You agree to indemnify and hold GUVI Incorporation, its affiliated companies, suppliers, partners, officers, contractors and employees harmless from any claim or demand made by any third party due to or arising out of (i) your actions in using the GUVI service, (ii) a claim that you, or any third party using your Credentials, infringed any intellectual property or other right of any person or organization using the GUVI service, or (iii) the violation of these Terms of Service by you or any third party using your Credentials. GUVI reserves the right, at our own expense, to assume the exclusive defense and control of any matter for which you are required to indemnify us, and you agree to cooperate with our defense of these claims. This indemnity shall not apply if a third party uses your Credentials due to an act or omission of GUVI.

Modifications to Prices. Prices of all fee-based GUVI services are subject to change. If they do change, the changes shall only take effect for any subsequent subscription periods you choose to purchase.

Miscellaneous:
These Terms of Service will remain in full force and effect while you use the GUVI service. Those terms that can continue to operate after you stop using the GUVI service (including without limitation your Content license to GUVI service and the General Terms in this Section), will survive after you stop using the GUVI service.

Statute of Limitations. You agree that regardless of any statute of law to the contrary or any claim or cause of action arising out of or related to use of GUVI services or the Terms of Service must be filed within one (1) year after such claim or cause of action arose or be forever barred.

Notices. You agree that GUVI may provide you with notices, including those regarding changes to the Terms of Service, by email, regular mail or by posting on the website hosting the GUVI services.

Waiver/Severability. The failure of GUVI to exercise or enforce any right or provision of the Terms of Service shall not constitute a waiver of such right or provision. If any provision of the Terms is found by a court of competent jurisdiction to be invalid, the parties nevertheless agree that the court should endeavor to give effect to the parties' intentions as reflected in the provision, and the other provisions of the Terms of Service shall remain in full force and effect.

Assignment. You may not assign your account to the GUVI service or any rights and licenses granted hereunder and such any assignment by you will be null and void.

Survivability. All sections of these Terms that by their respective nature should survive the cancellation or termination of these Terms of Service shall survive the termination or cancellation of these Terms of Service. You agree that breach of these provisions will cause irreparable harm and that accordingly GUVI may seek injunctive relief in addition to any other legal remedies under these Terms of Service at law or equity.

No Third Party Beneficiaries. You agree that except as otherwise expressly provided for in these Terms of Service, there shall be no third party beneficiaries to the Terms of Service.

Choice of Law/Forum. These Terms of Service (including any additional terms, conditions, policies and agreements incorporated herein) are the entire agreement between GUVI and you regarding the GUVI service. Any dispute arising from or related to these Terms of Service shall be subject to the ruling of an applicable court of competent jurisdiction, shall be settled by binding arbitration in County and administered by the Government and conducted by a sole arbitrator in accordance with Commercial Arbitration Rules. The Federal Arbitration Act, shall govern the arbitration to the exclusion of state laws inconsistent therewith or that would produce a different result, and any court having jurisdiction thereof may enter judgment on the award rendered by the arbitrator. Except as may be required by law or to the extent necessary in connection with a judicial challenge, or enforcement of an award, neither a party nor the arbitrator may disclose the existence, content, record, or results of an arbitration. Fourteen (14) calendar days before the hearing, the parties will exchange and provide to the arbitrator: (a) a list of witnesses they intend to call (including any experts) with a short description of the anticipated direct testimony of each witness and an estimate of the length thereof and (b) pre-marked copies of all exhibits they intend to use at the hearing. Deposition for discovery purposes shall not be permitted.

Contact Us
GUVI's Grievance Officer shall undertake all reasonable efforts to address your grievances at the earliest possible opportunity. For contact:

Grievance Officer: Mr.Muruganantham

Address: IITM Research park - phase 2, module #9, 3rd floor, D block, Kanagam Rd, Tharamani, Chennai, Tamil Nadu, India. 600113

Reach out to us on muruganantham@guvi.in, in case of any queries.

ZEN CLASS

LIVE CLASS

Full Stack Development
Automation & Testing
Data Science
UI/UX
DevOps
Data Engineering
Business Analytics with Digital Marketing
All Programs
Popular Courses

Python - IIT-M Pravartak Certified
Java
Mobile Hacking
C Programming
AWS
Angular
Dark Web
All Courses
Self-Paced Courses

Premium Pass
Paid Courses
Free Courses
Combos
Practice Platforms

CodeKata
WebKata
SQLKata
Debugging
IDE
Products

HackerKID
Placement Preparation
GUVI for Corporates
Studytonight
Resources

Success Stories
Learn Hub
Free Resources
New
Blog
Web Stories
Rewards
Refer a friend
Become an Affiliate
Company

Refund Policy
FAQs
Contact Us
About Us
guvi-logo
GUVI (Grab Your Vernacular Imprint) Geek Network Private Limited is a leading online learning and skills development company, incubated by IIT Madras and IIM Ahmedabad.

Established in 2014 and acquired by the HCL Group in 2022, GUVI is dedicated to providing effective and high-quality learning and skilling programs that transcend language barriers in technology education. GUVI today is trusted by over 3 million learners and 2000+ corporate partners.

Follow us on

facebook
instagram
linkedIn
twitter
telegram
youtube
app store
play store
Refer & Earn
Terms and Conditions
Privacy Policy
GUVI Geeks Network Pvt. Ltd.

Gift Icon
Unlock Your Exclusive Rewards!
You have exclusive rewards which you can unlock by earning Geekoins. Explore your rewards now!

Later
Categories

Education website
Contact info

097360 97320
Mobile

info@guvi.in
Email
Websites and social links

https://www.guvi.in/
Basic info

96% recommend (29 reviews)

Always open
Hours
About GUVI
Guvi is an IIT-M , IIM-A incubated company located at IITM Research Park Chennai. We help students easily master   any programming skills so that they can acquire and effectively apply the knowledge and skills necessary to thrive in their careers

We offer practice exercises, instructional videos, and a personalized learning dashboard that empower learners to study at their own pace.

We also offer in-class Full Stack Developer Course with Job assurance.

VIsit www.guvi.in for more info
Page transparency
Facebook is showing information to help you understand the purpose of this Page.

386984684662653
Page ID

8 March 2012
Creation date

Admin info
This Page can have multiple admins. They may have permission to post content, comment or send messages as the Page.

This Page is currently running ads.
Reviews
Privacy and legal info

GUVI is an integrated Edu-tech Platform for technology Skills providing a personalized & Interactive learning experience through personalized assessments, gamification & bite sized video contents in Vernacular Languages.  Also we help the learner find the right Jobs, all through our platform. See less
Impressum"""# Consistent sample text

# Use a tokenizer from the transformers library (e.g., a basic one like 'bert-base-uncased')
# You can choose a different tokenizer depending on your needs.
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Tokenize the text using simple split for word-level tokens
words = text.lower().split()
tokens = words # Using simple split as initial tokenization

# Build the vocabulary manually using Counter and a list from the consistent tokens
counter = Counter(tokens)
# Create vocabulary list
vocab_list = ['<unk>', '<pad>'] + [word for word, count in counter.most_common()]
# Create a mapping from token to index
vocab = {word: i for i, word in enumerate(vocab_list)}
# Set a default index for unknown tokens
default_index = vocab.get('<unk>', 0) # Use 0 as default if <unk> is not in vocab (shouldn't happen here)

# Numericalize the text using the consistent tokens and vocabulary
# Use the get method with a default value for out-of-vocabulary tokens
data = [vocab.get(token, default_index) for token in tokens]

# Convert data to tensors and create batches
# Adjust seq_length based on your data and model requirements
# Ensure seq_length is less than or equal to the length of 'data'
seq_length = 5 # Reduced seq_length for the small example

if len(data) < seq_length:
    print(f"Warning: data length ({len(data)}) is less than seq_length ({seq_length}). Cannot create batches.")
    inputs = torch.empty(0, seq_length - 1, dtype=torch.long)
    targets = torch.empty(0, dtype=torch.long)
else:
    def create_batches(data, seq_length):
        # Ensure full sequences are generated
        sequences = [data[i:i+seq_length] for i in range(0, len(data) - seq_length + 1)]
        inputs = torch.tensor([seq[:-1] for seq in sequences], dtype=torch.long)
        targets = torch.tensor([seq[-1] for seq in sequences], dtype=torch.long)
        return inputs, targets

    inputs, targets = create_batches(data, seq_length)

    # Split data into train and validation sets
    split_ratio = 0.8
    split_index = int(split_ratio * len(inputs))

    if split_index < len(inputs):
        train_data, val_data = inputs[:split_index], inputs[split_index:]
        train_targets, val_targets = targets[:split_index], targets[split_index:]
        print(f"Created {len(train_data)} training examples and {len(val_data)} validation examples.")
    else:
        print("Not enough data to create a validation set. Using all data for training.")
        train_data, val_data = inputs, torch.empty(0, seq_length - 1, dtype=torch.long)
        train_targets, val_targets = targets, torch.empty(0, dtype=torch.long)


# DataLoader
# Only create DataLoader if the respective dataset is not empty
train_loader = None
val_loader = None

if len(train_data) > 0:
    train_dataset = torch.utils.data.TensorDataset(train_data, train_targets)
    # Reduced batch size if needed for very small datasets
    batch_size = min(20, len(train_data)) if len(train_data) > 0 else 1
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    print(f"Train DataLoader created with {len(train_loader)} batches.")
else:
    print("Train dataset is empty. Cannot create train DataLoader.")

if len(val_data) > 0:
    val_dataset = torch.utils.data.TensorDataset(val_data, val_targets)
    # Reduced batch size if needed for very small datasets
    batch_size = min(20, len(val_data)) if len(val_data) > 0 else 1
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)
    print(f"Validation DataLoader created with {len(val_loader)} batches.")
else:
     print("Validation dataset is empty. Cannot create validation DataLoader.")


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Display some information about the processed data
print("\nSample Data and Vocabulary:")
print("Original Tokens:", tokens)
print("Numerical Data:", data)
print("Vocabulary:", vocab)
print("Inputs shape:", inputs.shape)
print("Targets shape:", targets.shape)

if train_loader and len(train_loader) > 0: # Check if loader exists and has batches
    for batch_inputs, batch_targets in train_loader:
        print("\nSample Train Batch:")
        print("Inputs shape:", batch_inputs.shape)
        print("Targets shape:", batch_targets.shape)
        print("Sample Inputs:", batch_inputs[0])
        print("Sample Targets:", batch_targets[0])
        break # Just show one batch
else:
    print("\nNo training batches to display.")

if val_loader and len(val_loader) > 0: # Check if loader exists and has batches
    for batch_inputs, batch_targets in val_loader:
        print("\nSample Validation Batch:")
        print("Inputs shape:", batch_inputs.shape)
        print("Targets shape:", batch_targets.shape)
        print("Sample Inputs:", batch_inputs[0])
        print("Sample Targets:", batch_targets[0])
        break # Just show one batch
else:
    print("\nNo validation batches to display.")

import torch.nn as nn
import torch.optim as optim

class RNNModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(RNNModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        embedded = self.embedding(x)
        output, hidden = self.rnn(embedded)
        output = self.fc(output[:, -1, :])
        return output

# Initialize the model, criterion, and optimizer
vocab_size = len(vocab)
embedding_dim = 100
hidden_dim = 100
output_dim = vocab_size

model = RNNModel(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

def train_epoch(loader, model, criterion, optimizer):
    model.train()
    total_loss = 0
    for inputs, targets in loader:
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        output = model(inputs)
        loss = criterion(output, targets)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)

def evaluate(loader, model, criterion):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            output = model(inputs)
            loss = criterion(output, targets)
            total_loss += loss.item()
    return total_loss / len(loader)

for epoch in range(1, 21):
    train_loss = train_epoch(train_loader, model, criterion, optimizer)
    val_loss = evaluate(val_loader, model, criterion)
    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')

def generate_text(model, seed_text, vocab, tokenizer, next_words=50, temperature=1.0):
    model.eval()
    # Get tokens by splitting, consistent with vocab creation in I10O1TRt9NJ8
    tokens = seed_text.lower().split()
    # Numericalize tokens using the provided vocab
    input_ids = torch.tensor([vocab.get(token, vocab.get('<unk>', 0)) for token in tokens], dtype=torch.long).unsqueeze(0).to(device)

    generated_text = seed_text
    with torch.no_grad():
        for _ in range(next_words):
            if input_ids.size(1) == 0: # Handle empty input after tokenization if seed_text is empty
                 break

            # Get the model output
            model_output = model(input_ids)

            # Check the number of dimensions of the output before slicing
            # The RNN model returns (batch_size, seq_len, hidden_dim) if seq_len > 1
            # and (batch_size, hidden_dim) if seq_len == 1 (or potentially different based on batch_first)
            if model_output.ndim == 3:
                # If 3 dimensions, get the output of the last time step
                logits = model_output[:, -1, :]
            elif model_output.ndim == 2:
                # If 2 dimensions (likely seq_len was 1), use the output directly
                logits = model_output
            else:
                print(f"Warning: Unexpected model output dimensions: {model_output.ndim}. Stopping generation.")
                break

            logits = logits.squeeze(0)  # Remove the batch dimension (assuming batch size 1)
            logits = logits / temperature
            probabilities = torch.nn.functional.softmax(logits, dim=-1)

            # Handle cases where probabilities might contain NaNs or be all zeros
            if torch.isnan(probabilities).any() or torch.sum(probabilities) == 0:
                 print("Warning: Probabilities are NaN or all zeros. Stopping generation.")
                 break

            # Sample the next token ID
            try:
                next_token_id = torch.multinomial(probabilities, num_samples=1).item()
            except RuntimeError as e:
                print(f"RuntimeError during multinomial sampling: {e}")
                print("Probabilities:", probabilities)
                break # Stop generation if sampling fails

            # Look up the token in the vocabulary. Use vocab_list for index to token mapping
            # Ensure the index is within the bounds of vocab_list
            if next_token_id < len(vocab_list):
                 next_token = vocab_list[next_token_id]
            else:
                 print(f"Warning: Generated token ID ({next_token_id}) is out of vocabulary bounds ({len(vocab_list)}). Using UNK.")
                 next_token = vocab_list[vocab.get('<unk>', 0)] # Use UNK token

            generated_text += ' ' + next_token

            # Prepare the input for the next step
            # Append the generated token ID to the input sequence
            next_input = torch.tensor([[next_token_id]], dtype=torch.long).to(device)
            input_ids = torch.cat((input_ids, next_input), dim=1)
            # Keep the input sequence length manageable (optional, but good for long sequences)
            # input_ids = input_ids[:, -seq_length:] # Keep only the last seq_length tokens

    return generated_text

# Assuming the model and vocab are available from previous cells
# model = ...
# vocab = ...
# tokenizer = ... # Use the tokenizer defined in I10O1TRt9NJ8 if needed for other tasks, but not for getting tokens here

seed_text = "guvi courses"
# Ensure vocab_list is accessible here if needed for index lookup
if 'vocab_list' not in globals():
     print("Error: vocab_list not found. Please ensure cell I10O1TRt9NJ8 has been executed and vocab_list is global or passed.")
     # You might need to re-run I10O1TRt9NJ8 or pass vocab_list to this function
else:
    generated_text = generate_text(model, seed_text, vocab, tokenizer, next_words=50, temperature=1.0)
    print(generated_text)

class LSTMModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super(LSTMModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        embedded = self.embedding(x)
        output, (hidden, cell) = self.lstm(embedded)
        output = self.fc(output[:, -1, :])
        return output

# Initialize the LSTM model
model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

# Train the LSTM model
for epoch in range(1, 201):
    train_loss = train_epoch(train_loader, model, criterion, optimizer)
    val_loss = evaluate(val_loader, model, criterion)
    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')

def generate_text(model, seed_text, vocab, tokenizer, next_words=50, temperature=1.0):
    model.eval()
    # Get tokens by splitting, consistent with vocab creation in I10O1TRt9NJ8
    tokens = seed_text.lower().split()
    # Numericalize tokens using the provided vocab
    input_ids = torch.tensor([vocab.get(token, vocab.get('<unk>', 0)) for token in tokens], dtype=torch.long).unsqueeze(0).to(device)

    generated_text = seed_text
    with torch.no_grad():
        for _ in range(next_words):
            if input_ids.size(1) == 0: # Handle empty input after tokenization if seed_text is empty
                 break

            # Get the model output
            model_output = model(input_ids)

            # Check the number of dimensions of the output before slicing
            # The model returns (batch_size, seq_len, output_dim) for RNN/LSTM
            if model_output.ndim == 3:
                # If 3 dimensions, get the output of the last time step
                logits = model_output[:, -1, :]
            elif model_output.ndim == 2:
                # If 2 dimensions (likely seq_len was 1), use the output directly
                logits = model_output
            else:
                print(f"Warning: Unexpected model output dimensions: {model_output.ndim}. Stopping generation.")
                break

            logits = logits.squeeze(0)  # Remove the batch dimension (assuming batch size 1)
            logits = logits / temperature
            probabilities = torch.nn.functional.softmax(logits, dim=-1)

            # Handle cases where probabilities might contain NaNs or be all zeros
            if torch.isnan(probabilities).any() or torch.sum(probabilities) == 0:
                 print("Warning: Probabilities are NaN or all zeros. Stopping generation.")
                 break


            # Sample the next token ID
            try:
                next_token_id = torch.multinomial(probabilities, num_samples=1).item()
            except RuntimeError as e:
                print(f"RuntimeError during multinomial sampling: {e}")
                print("Probabilities:", probabilities)
                break # Stop generation if sampling fails

            # Look up the token in the vocabulary list (vocab_list)
            # Ensure vocab_list is accessible here and index is within bounds
            if 'vocab_list' in globals() and next_token_id < len(vocab_list):
                next_token = vocab_list[next_token_id]
            else:
                print(f"Warning: Generated token ID ({next_token_id}) is out of vocabulary bounds or vocab_list not found. Using UNK.")
                next_token = vocab_list[vocab.get('<unk>', 0)] # Fallback to UNK token


            generated_text += ' ' + next_token

            # Prepare the input for the next step
            # Append the generated token ID to the input sequence
            next_input = torch.tensor([[next_token_id]], dtype=torch.long).to(device)
            input_ids = torch.cat((input_ids, next_input), dim=1)
            # Keep the input sequence length manageable (optional)
            # input_ids = input_ids[:, -seq_length:]


    return generated_text

# Assuming the model, vocab, vocab_list, and tokenizer are available from previous cells
# model = ...
# vocab = ...
# vocab_list = ... # Ensure vocab_list is defined in a previous cell and accessible
# tokenizer = ...

seed_text = "guvi courses"
# Ensure vocab_list is accessible before calling generate_text
if 'vocab_list' in globals():
    generated_text = generate_text(model, seed_text, vocab, tokenizer, next_words=50, temperature=1.0)
    print(generated_text)
else:
    print("Error: vocab_list not found. Please ensure cell I10O1TRt9NJ8 has been executed.")

import torch.nn as nn
import torch.optim as optim
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class TransformerModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, nhead, num_encoder_layers, hidden_dim, output_dim):
        super(TransformerModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.pos_encoder = PositionalEncoding(embedding_dim)
        encoder_layers = nn.TransformerEncoderLayer(embedding_dim, nhead, hidden_dim)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)
        self.fc = nn.Linear(embedding_dim, output_dim)

    def forward(self, x):
        embedded = self.embedding(x) * math.sqrt(embedding_dim)
        embedded = self.pos_encoder(embedded)
        output = self.transformer_encoder(embedded)
        output = self.fc(output[:, -1, :])
        return output

# Initialize the model, criterion, and optimizer
vocab_size = len(vocab)
embedding_dim = 200
nhead = 2
num_encoder_layers = 2
hidden_dim = 200
output_dim = vocab_size

model = TransformerModel(vocab_size, embedding_dim, nhead, num_encoder_layers, hidden_dim, output_dim).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

def train_epoch(loader, model, criterion, optimizer):
    model.train()
    total_loss = 0
    for inputs, targets in loader:
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        output = model(inputs)
        loss = criterion(output, targets)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)

def evaluate(loader, model, criterion):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            output = model(inputs)
            loss = criterion(output, targets)
            total_loss += loss.item()
    return total_loss / len(loader)

for epoch in range(1, 199):
    train_loss = train_epoch(train_loader, model, criterion, optimizer)
    val_loss = evaluate(val_loader, model, criterion)
    print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')

from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
model = AutoModelForCausalLM.from_pretrained("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

input_text = "Write me a poem about Machine Learning."
input_ids = tokenizer(input_text, return_tensors="pt")

outputs = model.generate(**input_ids)
print(tokenizer.decode(outputs[0]))

# Install the transformers library if not already installed
!pip install transformers

from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer

# Specify the model name or identifier
model_name = "gpt2"  # You can use other models like "gpt2-medium", "gpt2-large", "gpt2-xl", "gpt-neo-125M", etc.

# Load the pre-trained model and tokenizer
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Create a text generation pipeline
text_generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Generate text
prompt = "guvi courses"
generated_text = text_generator(prompt, max_length=100, do_sample=True, temperature=0.7)

print(generated_text[0]['generated_text'])

# Install the transformers library if not already installed
!pip install transformers

# Import the necessary library
from huggingface_hub import login

# Login using the token
login(token="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")

pip install transformers datasets torch fastapi uvicorn

pip install accelerate -U

import os
from transformers import BertTokenizer

def preprocess_data(input_file, output_file, tokenizer_name="openai/gpt-oss-20b"):
    tokenizer = BertTokenizer.from_pretrained(tokenizer_name)
    with open(input_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    with open(output_file, 'w', encoding='utf-8') as f:
        for line in lines:
            tokenized_line = tokenizer.tokenize(line)
            f.write(" ".join(tokenized_line) + "\n")

input_file = "/content/guvi source.txt"  # Your company-specific data file
output_file = "guvi source.txt"
preprocess_data(input_file, output_file)

import os
os.environ["WANDB_MODE"] = "disabled"

from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling

# Load pre-trained model and tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Create dataset
def load_dataset(file_path, tokenizer, block_size=128):
    return TextDataset(
        tokenizer=tokenizer,
        file_path=file_path,
        block_size=block_size,
    )

train_dataset = load_dataset(output_file, tokenizer)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=10_000,
    save_total_limit=2,
    # run_name=
)

# Initialize data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model and tokenizer
model.save_pretrained("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model")

!pip show accelerate

#!pip install transformers

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Load the fine-tuned model and tokenizer
model_name_or_path = "./fine_tuned_model"
model = GPT2LMHeadModel.from_pretrained(model_name_or_path)
tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Define the text generation function
def generate_text(model, tokenizer, seed_text, max_length=100, temperature=1.0, num_return_sequences=1):
    # Tokenize the input text
    input_ids = tokenizer.encode(seed_text, return_tensors='pt').to(device)

    # Generate text
    with torch.no_grad():
        output = model.generate(
            input_ids,
            max_length=max_length,
            temperature=temperature,
            num_return_sequences=num_return_sequences,
            do_sample=True,
            top_k=50,
            top_p=0.95,
        )

    # Decode the generated text
    generated_texts = []
    for i in range(num_return_sequences):
        generated_text = tokenizer.decode(output[i], skip_special_tokens=True)
        generated_texts.append(generated_text)

    return generated_texts

# Test the model
seed_text = "guvi is known for"
generated_texts = generate_text(model, tokenizer, seed_text, max_length=50, temperature=0.7, num_return_sequences=4)

for i, text in enumerate(generated_texts):
    print(f"Generated Text {i + 1}:\n{text}\n")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from transformers import GPT2LMHeadModel, GPT2Tokenizer
# import torch
# 
# # Load the fine-tuned model and tokenizer
# #model_name_or_path = "./fine_tuned_model"
# model_name_or_path = "gpt2"
# model = GPT2LMHeadModel.from_pretrained(model_name_or_path)
# tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)
# 
# # Move the model to GPU if available
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model.to(device)
# 
# # Define the text generation function
# def generate_text(model, tokenizer, seed_text, max_length=100, temperature=1.0, num_return_sequences=1):
#     input_ids = tokenizer.encode(seed_text, return_tensors='pt').to(device)
#     with torch.no_grad():
#         output = model.generate(
#             input_ids,
#             max_length=max_length,
#             temperature=temperature,
#             num_return_sequences=num_return_sequences,
#             do_sample=True,
#             top_k=50,
#             top_p=0.95,
#         )
#     generated_texts = [tokenizer.decode(output[i], skip_special_tokens=True) for i in range(num_return_sequences)]
#     return generated_texts
# 
# # Streamlit app
# st.title("Text Generation with GPT-2")
# st.write("This app generates text using a fine-tuned GPT-2 model. Enter a prompt and the model will generate a continuation.")
# 
# seed_text = st.text_input("Enter your prompt:", "Guvi is known for")
# max_length = st.slider("Max Length:", min_value=50, max_value=500, value=100)
# temperature = st.slider("Temperature:", min_value=0.1, max_value=2.0, value=1.0)
# 
# if st.button("Generate"):
#     with st.spinner("Generating text..."):
#         generated_texts = generate_text(model, tokenizer, seed_text, max_length, temperature)
#         for i, generated_text in enumerate(generated_texts):
#             st.subheader(f"Generated Text {i + 1}")
#             st.write(generated_text)

import gradio as gr
from deep_translator import GoogleTranslator
from langdetect import detect
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# =========================
# Translation Functions
# =========================
def to_english(text):
    lang = detect(text)
    if lang == "en":
        return text, "en"
    translated = GoogleTranslator(source=lang, target="en").translate(text)
    return translated, lang

def from_english(text, target_lang):
    if target_lang == "en":
        return text
    return GoogleTranslator(source="en", target=target_lang).translate(text)

# =========================
# Load GPT-2 Model
# =========================
def load_model():
    # 👉 Replace with your Hugging Face Hub model repo
    model_path = ""

    tokenizer = GPT2Tokenizer.from_pretrained(model_path)
    model = GPT2LMHeadModel.from_pretrained(model_path)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    return tokenizer, model, device

tokenizer, model, device = load_model()

# =========================
# Text Generation
# =========================
def generate_response(prompt, max_new_tokens=100):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        temperature=0.8
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# =========================
# Course Recommendation Logic
# =========================
def recommend_course(text):
    text = text.lower()
    if "python" in text:
        return "🐍 Python Programming – Beginner to Advanced"
    elif "data science" in text:
        return "📊 Data Science Master Program"
    elif "ai" in text or "machine learning" in text:
        return "🤖 AI & Machine Learning with Real Projects"
    elif "web" in text or "full stack" in text:
        return "🌐 Full Stack Web Development"
    elif "java" in text:
        return "☕ Java Programming Essentials"
    else:
        return None

# =========================
# Main Chat Function
# =========================
def multilingual_chat(user_input, history):
    eng_input, lang = to_english(user_input)

    # Check for course recommendation
    course = recommend_course(eng_input)
    if course:
        eng_response = f"I recommend you check out GUVI's course: {course}"
    else:
        eng_response = generate_response(eng_input)

    final_response = from_english(eng_response, lang)
    history.append((user_input, final_response))
    return history, history

# =========================
# Gradio UI
# =========================
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown(
        """
        # 🌐 GUVI Multilingual GPT-2 Chatbot
        🤖 Talk to me in **any language**.
        I'll translate, generate a response, and reply back in your language!
        """
    )

    chatbot = gr.Chatbot(height=400)
    msg = gr.Textbox(placeholder="💬 Type your question here...", label="Your Message")
    clear = gr.Button("🗑️ Clear Chat")

    state = gr.State([])  # keeps chat history

    msg.submit(multilingual_chat, [msg, state], [chatbot, state])
    clear.click(lambda: ([], []), None, [chatbot, state], queue=False)

# Launch app
if __name__ == "__main__":
    demo.launch()

!pip install deep_translator

!pip install langdetect



!pip install streamlit

!pip install faiss-cpu